
@inproceedings{merullo_circuit_2023,
	title = {Circuit {Component} {Reuse} {Across} {Tasks} in {Transformer} {Language} {Models}},
	url = {https://openreview.net/forum?id=fpoAYV6Wsk},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
	month = sep,
	year = {2023},
}

@inproceedings{wang_interpretability_2022,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 {Small}},
	url = {https://openreview.net/forum?id=NpsVSN6o4ul},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = sep,
	year = {2022},
}

@inproceedings{nanda_progress_2022,
	title = {Progress measures for grokking via mechanistic interpretability},
	url = {https://openreview.net/forum?id=9XFSbDPmdW},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
	month = sep,
	year = {2022},
}

@inproceedings{vig_investigating_2020,
	title = {Investigating gender bias in language models using causal mediation analysis},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
	month = may,
	year = {2020},
	pages = {12388--12401},
}

@inproceedings{mu_compositional_2020,
	title = {Compositional explanations of neurons},
	volume = {33},
	url = {https://arxiv.org/pdf/2006.14032},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mu, Jesse and Andreas, Jacob},
	month = may,
	year = {2020},
	pages = {17153--17163},
}

@inproceedings{jain_attention_2019,
	address = {Minneapolis, Minnesota},
	title = {Attention is not {Explanation}},
	url = {https://aclanthology.org/N19-1357},
	doi = {10.18653/v1/N19-1357},
	abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jain, Sarthak and Wallace, Byron C.},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {3543--3556},
}

@inproceedings{hanna_how_2023,
	title = {How does {GPT}-2 compute greater-than?: {Interpreting} mathematical abilities in a pre-trained language model},
	url = {https://openreview.net/forum?id=p4PckNQR8k},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
	month = may,
	year = {2023},
}

@misc{power_grokking_2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Datasets}},
	url = {https://arxiv.org/abs/2201.02177},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
}

@inproceedings{zhong_clock_2023,
	title = {The {Clock} and the {Pizza}: {Two} {Stories} in {Mechanistic} {Explanation} of {Neural} {Networks}},
	url = {https://arxiv.org/pdf/2306.17844},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhong, Zhiqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
	month = nov,
	year = {2023},
}

@misc{heimersheim_circuit_2023,
	type = {Blog},
	title = {A {Circuit} for {Python} {Docstrings} in a 4-{Layer} {Attention}-{Only} {Transformer}},
	url = {https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	urldate = {2024-06-17},
	journal = {AI Alignment Forum},
	author = {Heimersheim, Stefan and Janiak, Jett},
	month = feb,
	year = {2023},
}

@inproceedings{chughtai_toy_2023,
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	url = {https://arxiv.org/pdf/2302.03025},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	month = may,
	year = {2023},
}

@misc{belrose_eliciting_2023,
	title = {Eliciting {Latent} {Predictions} from {Transformers} with the {Tuned} {Lens}},
	url = {https://arxiv.org/abs/2303.08112},
	author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
	month = nov,
	year = {2023},
}

@inproceedings{conmy_towards_2023,
	title = {Towards {Automated} {Circuit} {Discovery} for {Mechanistic} {Interpretability}},
	url = {https://arxiv.org/abs/2304.14997},
	booktitle = {Thirty-{Seventh} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Conmy, Arthur and Parker-Mavor N., Augustine and Lynch, Aengus and Heimersheim, Stefan and Alonso-Garriga, Adria},
	month = oct,
	year = {2023},
}

@inproceedings{vilas_position_2024,
	title = {Position: {An} {Inner} {Interpretability} {Framework} for {AI} {Inspired} by {Lessons} from {Cognitive} {Science}},
	url = {https://arxiv.org/pdf/2406.01352},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	author = {Vilas, Martina G. and Adolfi, Federuci and Poeppel, David and Roig, Gemma},
	month = jun,
	year = {2024},
}

@misc{elhage_mathematical_2021,
	title = {A {Mathematical} {Framework} for {Transformer} {Circuits}},
	url = {https://transformer-circuits.pub/2021/framework/index.html},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and {DasSarma, Nova} and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	month = dec,
	year = {2021},
}

@misc{lieberum_does_2023,
	title = {Does {Circuit} {Analysis} {Interpretability} {Scale}? {Evidence} from {Multiple} {Choice} {Capabilities} in {Chinchilla}},
	url = {https://arxiv.org/abs/2307.09458},
	author = {Lieberum, Tom and Rahtz, Matthew and Kramár, János and Nanda, Neel and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
	month = jul,
	year = {2023},
}

@article{chan_causal_2022,
	title = {Causal scrubbing, a method for rigorously testing interpretability hypotheses},
	journal = {AI Alignment Forum},
	author = {Chan, Lawrence and Garriga-Alonso, Adrià and Goldwosky-Dill, Nicholas and Greenblatt, Ryan and Nitishinskaya, Jenny and Radhakrishnan, Ansh and Shlegeris, Buck and Thomas, Nate},
	month = dec,
	year = {2022},
}

@article{bricken_towards_2023,
	title = {Towards {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}},
	journal = {Transformer Circuits Thread},
	author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
	month = oct,
	year = {2023},
}

@misc{bills_language_2023,
	title = {Language models can explain neurons in language models},
	url = {https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html},
	author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
	month = may,
	year = {2023},
}

@misc{nainani_evaluating_2024,
	title = {Evaluating {Brain}-{Inspired} {Modular} {Training} in {Automated} {Circuit} {Discovery} for {Mechanistic} {Interpretability}},
	url = {https://arxiv.org/abs/2401.03646},
	author = {Nainani, Jatin},
	month = jan,
	year = {2024},
	note = {\_eprint: 2401.03646},
}

@inproceedings{meng_locating_2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {https://openreview.net/forum?id=-h6WAS6eE4},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Meng, Kevin and Bau, David and Andonian, Alex J. and Belinkov, Yonatan},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	month = oct,
	year = {2022},
}

@inproceedings{meng_mass-editing_2023,
	title = {Mass-{Editing} {Memory} in a {Transformer}},
	url = {https://openreview.net/forum?id=MkbcAHIYgyS},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J. and Belinkov, Yonatan and Bau, David},
	month = feb,
	year = {2023},
}

@misc{zhang_instilling_2024,
	title = {Instilling {Inductive} {Biases} with {Subnetworks}},
	url = {https://arxiv.org/abs/2310.10899},
	author = {Zhang, Enyan and Lepori, Michael A. and Pavlick, Ellie},
	month = oct,
	year = {2024},
	note = {\_eprint: 2310.10899},
}

@misc{akyurek_-context_2024,
	title = {In-{Context} {Language} {Learning}: {Architectures} and {Algorithms}},
	url = {https://arxiv.org/abs/2401.12973},
	author = {Akyürek, Ekin and Wang, Bailin and Kim, Yoon and Andreas, Jacob},
	month = jan,
	year = {2024},
	note = {\_eprint: 2401.12973},
}

@inproceedings{hase_does_2023,
	title = {Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3927bbdcf0e8d1fa8aa23c26f358a281-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
	month = may,
	year = {2023},
}

@inproceedings{chelombiev_adaptive_2019,
	title = {Adaptive {Estimators} {Show} {Information} {Compression} in {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=SkeZisA5t7},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chelombiev, Ivan and Houghton, Conor and O'Donnell, Cian},
	year = {2019},
}

@inproceedings{gilad-bachrach_information_2003,
	title = {An information theoretic tradeoff between complexity and accuracy},
	booktitle = {Learning {Theory} and {Kernel} {Machines}: 16th {Annual} {Conference} on {Learning} {Theory} and 7th {Kernel} {Workshop}, {COLT}/{Kernel} 2003, {Washington}, {DC}, {USA}, {August} 24-27, 2003. {Proceedings}},
	publisher = {Springer},
	author = {Gilad-Bachrach, Ran and Navot, Amir and Tishby, Naftali},
	year = {2003},
	pages = {595--609},
}

@article{asoodeh_bottleneck_2020,
	title = {Bottleneck problems: {An} information and estimation-theoretic view},
	volume = {22},
	number = {11},
	journal = {Entropy},
	author = {Asoodeh, Shahab and Calmon, Flavio P},
	year = {2020},
	note = {Publisher: MDPI},
	pages = {1325},
}

@article{galloway_bounding_2023,
	title = {Bounding generalization error with input compression: {An} empirical study with infinite-width networks},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=jbZEUtULft},
	journal = {Transactions on Machine Learning Research},
	author = {Galloway, Angus and Golubeva, Anna and Salem, Mahmoud and Nica, Mihai and Ioannou, Yani and Taylor, Graham W.},
	year = {2023},
}

@inproceedings{kolchinsky_caveats_2019,
	title = {Caveats for information bottleneck in deterministic scenarios},
	url = {https://openreview.net/forum?id=rke4HiAcY7},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kolchinsky, Artemy and Tracey, Brendan D. and Kuyk, Steven Van},
	year = {2019},
}

@inproceedings{tishby_deep_2015,
	title = {Deep learning and the information bottleneck principle},
	booktitle = {2015 {IEEE} {Information} {Theory} {Workshop} ({ITW})},
	publisher = {IEEE},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	year = {2015},
	pages = {1--5},
}

@article{aziznejad_deep_2020,
	title = {Deep neural networks with trainable activations and controlled {Lipschitz} constant},
	volume = {68},
	journal = {IEEE Transactions on Signal Processing},
	author = {Aziznejad, Shayan and Gupta, Harshit and Campos, Joaquim and Unser, Michael},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {4688--4699},
}

@inproceedings{liu_discrete-valued_2021,
	title = {Discrete-{Valued} {Neural} {Communication}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/10907813b97e249163587e6246612e21-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Dianbo and Lamb, Alex M and Kawaguchi, Kenji and Goyal, Anirudh and Sun, Chen and Mozer, Michael C and Bengio, Yoshua},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {2109--2121},
}

@inproceedings{fazlyab_efficient_2019,
	title = {Efficient and {Accurate} {Estimation} of {Lipschitz} {Constants} for {Deep} {Neural} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/95e1533eb1b20a97777749fb94fdb944-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{bialek_efficient_2006,
	title = {Efficient representation as a design principle for neural coding and computation},
	booktitle = {2006 {IEEE} international symposium on information theory},
	publisher = {IEEE},
	author = {Bialek, William and Van Steveninck, Rob R De Ruyter and Tishby, Naftali},
	year = {2006},
	pages = {659--663},
}

@article{achille_emergence_2018,
	title = {Emergence of invariance and disentanglement in deep representations},
	volume = {19},
	number = {50},
	journal = {Journal of Machine Learning Research},
	author = {Achille, Alessandro and Soatto, Stefano},
	year = {2018},
	pages = {1--34},
}

@inproceedings{goldfeld_estimating_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Estimating {Information} {Flow} in {Deep} {Neural} {Networks}},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/goldfeld19a.html},
	abstract = {We study the estimation of the mutual information I(X;T\_{\textbackslash}ell) between the input X to a deep neural network (DNN) and the output vector T\_{\textbackslash}ell of its {\textbackslash}ell-th hidden layer (an “internal representation”). Focusing on feedforward networks with fixed weights and noisy internal representations, we develop a rigorous framework for accurate estimation of I(X;T\_{\textbackslash}ell). By relating I(X;T\_{\textbackslash}ell) to information transmission over additive white Gaussian noise channels, we reveal that compression, i.e. reduction in I(X;T\_{\textbackslash}ell) over the course of training, is driven by progressive geometric clustering of the representations of samples from the same class. Experimental results verify this connection. Finally, we shift focus to purely deterministic DNNs, where I(X;T\_{\textbackslash}ell) is provably vacuous, and show that nevertheless, these models also cluster inputs belonging to the same class. The binning-based approximation of I(X;T\_{\textbackslash}ell) employed in past works to measure compression is identified as a measure of clustering, thus clarifying that these experiments were in fact tracking the same clustering phenomenon. Leveraging the clustering perspective, we provide new evidence that compression and generalization may not be causally related and discuss potential future research ideas.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Goldfeld, Ziv and Van Den Berg, Ewout and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {2299--2308},
}

@inproceedings{li_explanation_2023,
	address = {Toronto, Canada},
	title = {Explanation {Regeneration} via {Information} {Bottleneck}},
	url = {https://aclanthology.org/2023.findings-acl.765},
	doi = {10.18653/v1/2023.findings-acl.765},
	abstract = {Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superior generative capacity of large pretrained language models (PLM), recent work built on prompt engineering enables explanations generated without specific training. However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues. To discard the dross and take the essence of current PLM's results, we propose to produce sufficient and concise explanations via the information bottleneck (EIB) theory. EIB regenerates explanations by polishing the single-pass output of PLM but retaining the information that supports the contents being explained by balancing two information bottleneck objectives. Experiments on two different tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Li, Qintong and Wu, Zhiyong and Kong, Lingpeng and Bi, Wei},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {12081--12102},
}

@inproceedings{alemi_fixing_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Fixing a {Broken} {ELBO}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/alemi18a.html},
	abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Alemi, Alexander and Poole, Ben and Fischer, Ian and Dillon, Joshua and Saurous, Rif A. and Murphy, Kevin},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {159--168},
}

@article{painsky_gaussian_2018,
	title = {Gaussian {Lower} {Bound} for the {Information} {Bottleneck} {Limit}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/17-398.html},
	number = {213},
	journal = {Journal of Machine Learning Research},
	author = {Painsky, Amichai and Tishby, Naftali},
	year = {2018},
	pages = {1--29},
}

@inproceedings{kawaguchi_how_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {How {Does} {Information} {Bottleneck} {Help} {Deep} {Learning}?},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/kawaguchi23a.html},
	abstract = {Numerous deep learning algorithms have been inspired by and understood via the notion of information bottleneck, where unnecessary information is (often implicitly) minimized while task-relevant information is maximized. However, a rigorous argument for justifying why it is desirable to control information bottlenecks has been elusive. In this paper, we provide the first rigorous learning theory for justifying the benefit of information bottleneck in deep learning by mathematically relating information bottleneck to generalization errors. Our theory proves that controlling information bottleneck is one way to control generalization errors in deep learning, although it is not the only or necessary way. We investigate the merit of our new mathematical findings with experiments across a range of architectures and learning settings. In many cases, generalization errors are shown to correlate with the degree of information bottleneck: i.e., the amount of the unnecessary information at hidden layers. This paper provides a theoretical foundation for current and future methods through the lens of information bottleneck. Our new generalization bounds scale with the degree of information bottleneck, unlike the previous bounds that scale with the number of parameters, VC dimension, Rademacher complexity, stability or robustness. Our code is publicly available at: https://github.com/xu-ji/information-bottleneck},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kawaguchi, Kenji and Deng, Zhun and Ji, Xu and Huang, Jiaoyang},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {16049--16096},
}

@article{hafez_kolahi_information_2018,
	title = {Information {Bottleneck} and its {Applications} in {Deep} {Learning}},
	volume = {3},
	number = {23},
	journal = {Journal of Information Systems and Telecommunication (JIST)},
	author = {Hafez Kolahi, Hassan and Kasaei, Shohreh},
	year = {2018},
	note = {Publisher: RICEST},
	pages = {119},
}

@inproceedings{chechik_information_2003,
	title = {Information {Bottleneck} for {Gaussian} {Variables}},
	volume = {16},
	url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7e05d6f828574fbc975a896b25bb011e-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Chechik, Gal and Globerson, Amir and Tishby, Naftali and Weiss, Yair},
	editor = {Thrun, S. and Saul, L. and Schölkopf, B.},
	year = {2003},
}

@inproceedings{lorenzen_information_2022,
	title = {Information {Bottleneck}: {Exact} {Analysis} of ({Quantized}) {Neural} {Networks}},
	url = {https://openreview.net/forum?id=kF9DZQQrU0w},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lorenzen, Stephan Sloth and Igel, Christian and Nielsen, Mads},
	year = {2022},
}

@inproceedings{xu_information-theoretic_2017,
	title = {Information-theoretic analysis of generalization capability of learning algorithms},
	volume = {30},
	booktitle = {Advances in neural information processing systems},
	author = {Xu, Aolin and Raginsky, Maxim},
	year = {2017},
}

@article{braun_information-theoretic_2014,
	title = {Information-theoretic bounded rationality and ε-optimality},
	volume = {16},
	number = {8},
	journal = {Entropy},
	author = {Braun, Daniel A and Ortega, Pedro A},
	year = {2014},
	note = {Publisher: MDPI},
	pages = {4662--4676},
}

@inproceedings{li_invariant_2022,
	title = {Invariant information bottleneck for domain generalization},
	volume = {36},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Bo and Shen, Yifei and Wang, Yezhen and Zhu, Wenzhen and Li, Dongsheng and Keutzer, Kurt and Zhao, Han},
	year = {2022},
	note = {Issue: 7},
	pages = {7399--7407},
}

@inproceedings{wu_learnability_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learnability for the {Information} {Bottleneck}},
	volume = {115},
	url = {https://proceedings.mlr.press/v115/wu20b.html},
	abstract = {The Information Bottleneck (IB) method (Tishby et al. (2000)) provides an insightful and principled approach for balancing compression and prediction for representation learning. The IB objective I (X ; Z ) − βI(Y ; Z) employs a Lagrange multiplier β to tune this trade-off. However, in practice, not only is β chosen empirically without theoretical guidance, there is also a lack of theoretical understanding between β, learnability, the intrinsic nature of the dataset and model capacity. In this paper, we show that if β is improperly chosen, learning cannot happen – the trivial representation P(Z{\textbar}X) = P(Z) becomes the global minimum of the IB objective. We show how this can be avoided, by identifying a sharp phase transition between the unlearnable and the learnable which arises as β is varied. This phase transition defines the concept of IB-Learnability. We prove several sufficient conditions for IB-Learnability, which provides theoretical guidance for choosing a good β. We further show that IB-learnability is determined by the largest confident, typical, and imbalanced subset of the examples (the conspicuous subset), and discuss its relation with model capacity. We give practical algorithms to estimate the minimum β for a given dataset. We also empirically demonstrate our theoretical conditions with analyses of synthetic datasets, MNIST, and CIFAR10.},
	booktitle = {Proceedings of {The} 35th {Uncertainty} in {Artificial} {Intelligence} {Conference}},
	publisher = {PMLR},
	author = {Wu, Tailin and Fischer, Ian and Chuang, Isaac L. and Tegmark, Max},
	editor = {Adams, Ryan P. and Gogate, Vibhav},
	month = jul,
	year = {2020},
	pages = {1050--1060},
}

@inproceedings{bassily_learners_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learners that {Use} {Little} {Information}},
	volume = {83},
	url = {https://proceedings.mlr.press/v83/bassily18a.html},
	abstract = {We study learning algorithms that are restricted to using a small amount of information from their input sample. We introduce a category of learning algorithms we term {\textbackslash}em d-bit information learners, which are algorithms whose output conveys at most d bits of information of their input. A central theme in this work is that such algorithms generalize. We focus on the learning capacity of these algorithms, and prove sample complexity bounds with tight dependencies on the confidence and error parameters. We also observe connections with well studied notions such as sample compression schemes, Occam’s razor, PAC-Bayes and differential privacy. We discuss an approach that allows us to prove upper bounds on the amount of information that algorithms reveal about their inputs, and also provide a lower bound by showing a simple concept class for which every (possibly randomized) empirical risk minimizer must reveal a lot of information. On the other hand, we show that in the distribution-dependent setting every VC class has empirical risk minimizers that do not reveal a lot of information.},
	booktitle = {Proceedings of {Algorithmic} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Bassily, Raef and Moran, Shay and Nachum, Ido and Shafer, Jonathan and Yehudayoff, Amir},
	editor = {Janoos, Firdaus and Mohri, Mehryar and Sridharan, Karthik},
	month = apr,
	year = {2018},
	pages = {25--55},
}

@article{shamir_learning_2010,
	title = {Learning and generalization with the information bottleneck},
	volume = {411},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S030439751000201X},
	doi = {https://doi.org/10.1016/j.tcs.2010.04.006},
	abstract = {The Information Bottleneck is an information theoretic framework that finds concise representations for an ‘input’ random variable that are as relevant as possible for an ‘output’ random variable. This framework has been used successfully in various supervised and unsupervised applications. However, its learning theoretic properties and justification remained unclear as it differs from standard learning models in several crucial aspects, primarily its explicit reliance on the joint input–output distribution. In practice, an empirical plug-in estimate of the underlying distribution has been used, so far without any finite sample performance guarantees. In this paper we present several formal results that address these difficulties. We prove several finite sample bounds, which show that the information bottleneck can provide concise representations with good generalization, based on smaller sample sizes than needed to estimate the underlying distribution. The bounds are non-uniform and adaptive to the complexity of the specific model chosen. Based on these results, we also present a preliminary analysis on the possibility of analyzing the information bottleneck method as a learning algorithm in the familiar performance-complexity tradeoff framework. In addition, we formally describe the connection between the information bottleneck and minimal sufficient statistics.},
	number = {29},
	journal = {Theoretical Computer Science},
	author = {Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
	year = {2010},
	keywords = {Information bottleneck, Information theory, Statistical learning theory, Sufficient statistics},
	pages = {2696--2711},
}

@inproceedings{federici_learning_2020,
	title = {Learning {Robust} {Representations} via {Multi}-{View} {Information} {Bottleneck}},
	url = {https://openreview.net/forum?id=B1xwcyHFDr},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Federici, Marco and Dutta, Anjan and Forré, Patrick and Kushman, Nate and Akata, Zeynep},
	year = {2020},
}

@article{geiger_information_2021,
	title = {On information plane analyses of neural network classifiers—{A} review},
	volume = {33},
	number = {12},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Geiger, Bernhard C},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {7039--7051},
}

@article{zaidi_information_2020,
	title = {On the {Information} {Bottleneck} {Problems}: {Models}, {Connections}, {Applications} and {Information} {Theoretic} {Views}},
	volume = {22},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/22/2/151},
	doi = {10.3390/e22020151},
	abstract = {This tutorial paper focuses on the variants of the bottleneck problem taking an information theoretic perspective and discusses practical methods to solve it, as well as its connection to coding and learning aspects. The intimate connections of this setting to remote source-coding under logarithmic loss distortion measure, information combining, common reconstruction, the Wyner–Ahlswede–Korner problem, the efficiency of investment information, as well as, generalization, variational inference, representation learning, autoencoders, and others are highlighted. We discuss its extension to the distributed information bottleneck problem with emphasis on the Gaussian model and highlight the basic connections to the uplink Cloud Radio Access Networks (CRAN) with oblivious processing. For this model, the optimal trade-offs between relevance (i.e., information) and complexity (i.e., rates) in the discrete and vector Gaussian frameworks is determined. In the concluding outlook, some interesting problems are mentioned such as the characterization of the optimal inputs (“features”) distributions under power limitations maximizing the “relevance” for the Gaussian information bottleneck, under “complexity” constraints.},
	number = {2},
	journal = {Entropy},
	author = {Zaidi, Abdellatif and Estella-Aguerri, Iñaki and Shamai (Shitz), Shlomo},
	year = {2020},
}

@misc{shwartz-ziv_opening_2017,
	title = {Opening the {Black} {Box} of {Deep} {Neural} {Networks} via {Information}},
	author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	year = {2017},
	note = {\_eprint: 1703.00810},
}

@inproceedings{wang_pac-bayes_2022,
	title = {{PAC}-{Bayes} {Information} {Bottleneck}},
	url = {https://openreview.net/forum?id=iLHOIDsPv1P},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wang, Zifeng and Huang, Shao-Lun and Kuruoglu, Ercan Engin and Sun, Jimeng and Chen, Xi and Zheng, Yefeng},
	year = {2022},
}

@inproceedings{ngampruetikorn_perturbation_2021,
	title = {Perturbation {Theory} for the {Information} {Bottleneck}},
	url = {https://openreview.net/forum?id=A2HvBPoSBMs},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ngampruetikorn, Vudtiwat and Schwab, David J.},
	editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
	year = {2021},
}

@inproceedings{wu_phase_2020,
	title = {Phase {Transitions} for the {Information} {Bottleneck} in {Representation} {Learning}},
	url = {https://openreview.net/forum?id=HJloElBYvB},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wu, Tailin and Fischer, Ian},
	year = {2020},
}

@article{bartlett_rademacher_2002,
	title = {Rademacher and {Gaussian} complexities: {Risk} bounds and structural results},
	volume = {3},
	number = {Nov},
	journal = {Journal of Machine Learning Research},
	author = {Bartlett, Peter L and Mendelson, Shahar},
	year = {2002},
	pages = {463--482},
}

@inproceedings{steinke_reasoning_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Reasoning {About} {Generalization} via {Conditional} {Mutual} {Information}},
	volume = {125},
	url = {https://proceedings.mlr.press/v125/steinke20a.html},
	abstract = {We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e., the training data) can be recognized given the output (i.e., the trained model) of the learning algorithm. We show that bounds on CMI can be obtained from VC dimension, compression schemes, differential privacy, and other methods. We then show that bounded CMI implies various forms of generalization.},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Steinke, Thomas and Zakynthinou, Lydia},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	month = jul,
	year = {2020},
	pages = {3437--3452},
}

@inproceedings{kawaguchi_robustness_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Robustness {Implies} {Generalization} via {Data}-{Dependent} {Generalization} {Bounds}},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/kawaguchi22a.html},
	abstract = {This paper proves that robustness implies generalization via data-dependent generalization bounds. As a result, robustness and generalization are shown to be connected closely in a data-dependent manner. Our bounds improve previous bounds in two directions, to solve an open problem that has seen little development since 2010. The first is to reduce the dependence on the covering number. The second is to remove the dependence on the hypothesis space. We present several examples, including ones for lasso and deep learning, in which our bounds are provably preferable. The experiments on real-world data and theoretical models demonstrate near-exponential improvements in various situations. To achieve these improvements, we do not require additional assumptions on the unknown distribution; instead, we only incorporate an observable and computable property of the training samples. A key technical innovation is an improved concentration bound for multinomial random variables that is of independent interest beyond robustness and generalization.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kawaguchi, Kenji and Deng, Zhun and Luh, Kyle and Huang, Jiaoyang},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	pages = {10866--10894},
}

@article{fischer_conditional_2020,
	title = {The {Conditional} {Entropy} {Bottleneck}},
	volume = {22},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/22/9/999},
	doi = {10.3390/e22090999},
	abstract = {Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.},
	number = {9},
	journal = {Entropy},
	author = {Fischer, Ian},
	year = {2020},
}

@article{piran_dual_2020,
	title = {The {Dual} {Information} {Bottleneck}},
	journal = {arXiv preprint arXiv:2006.04641},
	author = {Piran, Zoe and Shwartz-Ziv, Ravid and Tishby, Naftali},
	year = {2020},
}

@misc{tishby_information_2000,
	title = {The information bottleneck method},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	year = {2000},
	note = {\_eprint: physics/0004057},
}

@article{goldfeld_information_2020,
	title = {The information bottleneck problem and its applications in machine learning},
	volume = {1},
	number = {1},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Goldfeld, Ziv and Polyanskiy, Yury},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {19--38},
}

@inproceedings{harremoes_information_2007,
	title = {The information bottleneck revisited or how to choose a good distortion measure},
	booktitle = {2007 {IEEE} {International} {Symposium} on {Information} {Theory}},
	publisher = {IEEE},
	author = {Harremoës, Peter and Tishby, Naftali},
	year = {2007},
	pages = {566--570},
}

@inproceedings{vera_role_2018,
	title = {The role of the information bottleneck in representation learning},
	booktitle = {2018 {IEEE} international symposium on information theory ({ISIT})},
	publisher = {IEEE},
	author = {Vera, Matias and Piantanida, Pablo and Vega, Leonardo Rey},
	year = {2018},
	pages = {1580--1584},
}

@inproceedings{deng_toward_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Toward {Better} {Generalization} {Bounds} with {Locally} {Elastic} {Stability}},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/deng21b.html},
	abstract = {Algorithmic stability is a key characteristic to ensure the generalization ability of a learning algorithm. Among different notions of stability, {\textbackslash}emphuniform stability is arguably the most popular one, which yields exponential generalization bounds. However, uniform stability only considers the worst-case loss change (or so-called sensitivity) by removing a single data point, which is distribution-independent and therefore undesirable. There are many cases that the worst-case sensitivity of the loss is much larger than the average sensitivity taken over the single data point that is removed, especially in some advanced models such as random feature models or neural networks. Many previous works try to mitigate the distribution independent issue by proposing weaker notions of stability, however, they either only yield polynomial bounds or the bounds derived do not vanish as sample size goes to infinity. Given that, we propose {\textbackslash}emphlocally elastic stability as a weaker and distribution-dependent stability notion, which still yields exponential generalization bounds. We further demonstrate that locally elastic stability implies tighter generalization bounds than those derived based on uniform stability in many situations by revisiting the examples of bounded support vector machines, regularized least square regressions, and stochastic gradient descent.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Deng, Zhun and He, Hangfeng and Su, Weijie},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {2590--2600},
}

@article{kirsch_unifying_2022,
	title = {Unifying {Approaches} in {Active} {Learning} and {Active} {Sampling} via {Fisher} {Information} and {Information}-{Theoretic} {Quantities}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=UVDAKQANOW},
	journal = {Transactions on Machine Learning Research},
	author = {Kirsch, Andreas and Gal, Yarin},
	year = {2022},
}

@misc{kirsch_unpacking_2021,
	title = {Unpacking {Information} {Bottlenecks}: {Surrogate} {Objectives} for {Deep} {Learning}},
	url = {https://openreview.net/forum?id=5rc0K0ezhqI},
	author = {Kirsch, Andreas and Lyle, Clare and Gal, Yarin},
	year = {2021},
}

@inproceedings{kingma_variational_2015,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	booktitle = {Proceedings of 3rd {International} {Conference} on {Learning} {Representations}},
	author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
	year = {2014},
}

@inproceedings{hein_formal_2017,
	title = {Formal {Guarantees} on the {Robustness} of a {Classifier} against {Adversarial} {Manipulation}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hein, Matthias and Andriushchenko, Maksym},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{cohen_certified_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/cohen19c.html},
	abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with L2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {1310--1320},
}

@article{stone_generalized_1948,
	title = {The {Generalized} {Weierstrass} {Approximation} {Theorem}},
	volume = {21},
	issn = {0025570X, 19300980},
	url = {http://www.jstor.org/stable/3029337},
	number = {5},
	urldate = {2024-06-23},
	journal = {Mathematics Magazine},
	author = {Stone, Marshall H.},
	year = {1948},
	note = {Publisher: Mathematical Association of America},
	pages = {237--254},
}

@inproceedings{chao_jailbreaking_2023,
	title = {Jailbreaking {Black} {Box} {Large} {Language} {Models} in {Twenty} {Queries}},
	url = {https://openreview.net/forum?id=rYWD5TMaLj},
	booktitle = {R0-{FoMo}:{Robustness} of {Few}-shot and {Zero}-shot {Learning} in {Large} {Foundation} {Models}},
	author = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J. and Wong, Eric},
	year = {2023},
}

@inproceedings{dong_boosting_2018,
	title = {Boosting {Adversarial} {Attacks} {With} {Momentum}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	month = jun,
	year = {2018},
}

@inproceedings{deng_universal_2020,
	title = {Universal adversarial attack via enhanced projected gradient descent},
	booktitle = {2020 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {Deng, Yingpeng and Karam, Lina J},
	year = {2020},
	pages = {1241--1245},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, George},
	year = {1989},
	note = {Publisher: Springer},
	pages = {303--314},
}

@inproceedings{lu_expressive_2017,
	title = {The {Expressive} {Power} of {Neural} {Networks}: {A} {View} from the {Width}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{yoo_towards_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Towards {Improving} {Adversarial} {Training} of {NLP} {Models}},
	url = {https://aclanthology.org/2021.findings-emnlp.81},
	doi = {10.18653/v1/2021.findings-emnlp.81},
	abstract = {Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP models, which we name Attacking to Training (A2T). The core part of A2T is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use A2T to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results empirically show that it is possible to train robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with A2T can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of word substitution attacks. Furthermore, we show that A2T can improve NLP models' standard accuracy, cross-domain generalization, and interpretability.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yoo, Jin Yong and Qi, Yanjun},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {945--956},
}

@inproceedings{madry_towards_2018,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {https://openreview.net/forum?id=rJzIBfZAb},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = {2018},
}

@inproceedings{wong_fast_2020,
	title = {Fast is better than free: {Revisiting} adversarial training},
	url = {https://openreview.net/forum?id=BJx040EFvH},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wong, Eric and Rice, Leslie and Kolter, J. Zico},
	year = {2020},
}

@inproceedings{shafahi_adversarial_2019,
	title = {Adversarial training for free!},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{bai_recent_2021,
	title = {Recent {Advances} in {Adversarial} {Training} for {Adversarial} {Robustness}},
	url = {https://doi.org/10.24963/ijcai.2021/591},
	doi = {10.24963/ijcai.2021/591},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI}-21},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
	editor = {Zhou, Zhi-Hua},
	month = aug,
	year = {2021},
	pages = {4312--4321},
}

@article{miyato_virtual_2019,
	title = {Virtual {Adversarial} {Training}: {A} {Regularization} {Method} for {Supervised} and {Semi}-{Supervised} {Learning}},
	volume = {41},
	doi = {10.1109/TPAMI.2018.2858821},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Miyato, Takeru and Maeda, Shin-Ichi and Koyama, Masanori and Ishii, Shin},
	year = {2019},
	keywords = {adversarial examples, adversarial training, Artificial neural networks, Computational modeling, Data models, deep learning, Perturbation methods, robustness, Robustness, Semi-supervised learning, Semisupervised learning, supervised learning, Training},
	pages = {1979--1993},
}

@inproceedings{cranko_generalised_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Generalised {Lipschitz} {Regularisation} {Equals} {Distributional} {Robustness}},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/cranko21a.html},
	abstract = {The problem of adversarial examples has highlighted the need for a theory of regularisation that is general enough to apply to exotic function classes, such as universal approximators. In response, we have been able to significantly sharpen existing results regarding the relationship between distributional robustness and regularisation, when defined with a transportation cost uncertainty set. The theory allows us to characterise the conditions under which the distributional robustness equals a Lipschitz-regularised model, and to tightly quantify, for the first time, the slackness under very mild assumptions. As a theoretical application we show a new result explicating the connection between adversarial learning and distributional robustness. We then give new results for how to achieve Lipschitz regularisation of kernel classifiers, which are demonstrated experimentally.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cranko, Zac and Shi, Zhan and Zhang, Xinhua and Nock, Richard and Kornblith, Simon},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {2178--2188},
}

@inproceedings{tsuzuku_lipschitz-margin_2018,
	title = {Lipschitz-{Margin} {Training}: {Scalable} {Certification} of {Perturbation} {Invariance} for {Deep} {Neural} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/485843481a7edacbfce101ecb1e4d2a8-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@inproceedings{alzantot_generating_2018,
	address = {Brussels, Belgium},
	title = {Generating {Natural} {Language} {Adversarial} {Examples}},
	url = {https://aclanthology.org/D18-1316},
	doi = {10.18653/v1/D18-1316},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97\% and 70\%, respectively. We additionally demonstrate that 92.3\% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {2890--2896},
}

@inproceedings{carlini_are_2023,
	title = {Are aligned neural networks adversarially aligned?},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/c1f0b856a35986348ab3414177266f75-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A. and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {61478--61500},
}

@inproceedings{wei_jailbroken_2023,
	title = {Jailbroken: {How} {Does} {LLM} {Safety} {Training} {Fail}?},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {80079--80110},
}

@inproceedings{li_contextualized_2021,
	address = {Online},
	title = {Contextualized {Perturbation} for {Textual} {Adversarial} {Attack}},
	url = {https://aclanthology.org/2021.naacl-main.400},
	doi = {10.18653/v1/2021.naacl-main.400},
	abstract = {Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Dianqi and Zhang, Yizhe and Peng, Hao and Chen, Liqun and Brockett, Chris and Sun, Ming-Ting and Dolan, Bill},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {5053--5069},
}

@inproceedings{wallace_universal_2019,
	address = {Hong Kong, China},
	title = {Universal {Adversarial} {Triggers} for {Attacking} and {Analyzing} {NLP}},
	url = {https://aclanthology.org/D19-1221},
	doi = {10.18653/v1/D19-1221},
	abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {2153--2162},
}

@article{pauli_training_2021,
	title = {Training robust neural networks using {Lipschitz} bounds},
	volume = {6},
	journal = {IEEE Control Systems Letters},
	author = {Pauli, Patricia and Koch, Anne and Berberich, Julian and Kohler, Paul and Allgöwer, Frank},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {121--126},
}

@misc{zou_universal_2023,
	title = {Universal and {Transferable} {Adversarial} {Attacks} on {Aligned} {Language} {Models}},
	author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
	year = {2023},
	note = {\_eprint: 2307.15043},
}

@inproceedings{wang_adversarial_2021,
	title = {Adversarial {GLUE}: {A} {Multi}-{Task} {Benchmark} for {Robustness} {Evaluation} of {Language} {Models}},
	url = {https://openreview.net/forum?id=GF9cSKI3A_q},
	booktitle = {Thirty-fifth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track} ({Round} 2)},
	author = {Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
	year = {2021},
}

@inproceedings{nie_adversarial_2020,
	address = {Online},
	title = {Adversarial {NLI}: {A} {New} {Benchmark} for {Natural} {Language} {Understanding}},
	url = {https://aclanthology.org/2020.acl-main.441},
	doi = {10.18653/v1/2020.acl-main.441},
	abstract = {We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {4885--4901},
}

@inproceedings{maas_learning_2011,
	address = {Portland, Oregon, USA},
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	url = {http://www.aclweb.org/anthology/P11-1015},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	month = jun,
	year = {2011},
	pages = {142--150},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@misc{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	publisher = {OpenAI},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and {others}},
	year = {2018},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/20-074.html},
	number = {140},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	pages = {1--67},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{ruan_reachability_2018,
	series = {{IJCAI}'18},
	title = {Reachability analysis of deep neural networks with provable guarantees},
	isbn = {978-0-9992411-2-7},
	abstract = {Verifying correctness of deep neural networks (DNNs) is challenging. We study a generic reachability problem for feed-forward DNNs which, for a given set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency, scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.},
	booktitle = {Proceedings of the 27th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Ruan, Wenjie and Huang, Xiaowei and Kwiatkowska, Marta},
	year = {2018},
	note = {Place: Stockholm, Sweden},
	pages = {2651--2659},
}

@misc{liu_adversarial_2020,
	title = {Adversarial {Training} for {Large} {Neural} {Language} {Models}},
	author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
	year = {2020},
	note = {\_eprint: 2004.08994},
}

@inproceedings{kim_lipschitz_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {The {Lipschitz} {Constant} of {Self}-{Attention}},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/kim21i.html},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Hyunjik and Papamakarios, George and Mnih, Andriy},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {5562--5571},
}

@inproceedings{yang_closer_2020,
	title = {A {Closer} {Look} at {Accuracy} vs. {Robustness}},
	volume = {33},
	booktitle = {Advances in neural information processing systems},
	author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
	year = {2020},
	pages = {8588--8601},
}

@article{nemirovskij_problem_1983,
	title = {Problem complexity and method efficiency in optimization},
	author = {Nemirovskij, Arkadij Semenovič and Yudin, David Borisovich},
	year = {1983},
	note = {Publisher: Wiley-Interscience},
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	journal = {arXiv preprint arXiv:1312.6199},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2013},
}

@article{biderman_emergent_2023,
	title = {Emergent and predictable memorization in large language models},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Biderman, Stella and Prashanth, Usvsn and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
	year = {2023},
}

@inproceedings{carlini_quantifying_2023,
	title = {Quantifying {Memorization} {Across} {Neural} {Language} {Models}},
	url = {https://openreview.net/forum?id=TatRHT_1cK},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
	year = {2023},
}

@inproceedings{hendrycks_natural_2021,
	title = {Natural {Adversarial} {Examples}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	month = jun,
	year = {2021},
	pages = {15262--15271},
}

@inproceedings{eykholt_robust_2018,
	title = {Robust {Physical}-{World} {Attacks} on {Deep} {Learning} {Visual} {Classification}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
	month = jun,
	year = {2018},
}

@inproceedings{garg_bae_2020,
	address = {Online},
	title = {{BAE}: {BERT}-based {Adversarial} {Examples} for {Text} {Classification}},
	url = {https://aclanthology.org/2020.emnlp-main.498},
	doi = {10.18653/v1/2020.emnlp-main.498},
	abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Garg, Siddhant and Ramakrishnan, Goutham},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {6174--6181},
}

@inproceedings{jin_is_2020,
	title = {Is {BERT} really robust? a strong baseline for natural language attack on text classification and entailment},
	volume = {34},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
	year = {2020},
	pages = {8018--8025},
}

@inproceedings{li_bert-attack_2020,
	address = {Online},
	title = {{BERT}-{ATTACK}: {Adversarial} {Attack} {Against} {BERT} {Using} {BERT}},
	url = {https://aclanthology.org/2020.emnlp-main.500},
	doi = {10.18653/v1/2020.emnlp-main.500},
	abstract = {Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {6193--6202},
}

@inproceedings{ilyas_black-box_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Black-box {Adversarial} {Attacks} with {Limited} {Queries} and {Information}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/ilyas18a.html},
	abstract = {Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {2137--2146},
}

@inproceedings{usama_towards_2019,
	title = {Towards robust neural networks with lipschitz continuity},
	booktitle = {Digital {Forensics} and {Watermarking}: 17th {International} {Workshop}, {IWDW} 2018, {Jeju} {Island}, {Korea}, {October} 22-24, 2018, {Proceedings} 17},
	publisher = {Springer},
	author = {Usama, Muhammad and Chang, Dong Eui},
	year = {2019},
	pages = {373--389},
}

@inproceedings{bartlett_spectrally-normalized_2017,
	title = {Spectrally-normalized margin bounds for neural networks},
	volume = {30},
	booktitle = {Advances in neural information processing systems},
	author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
	year = {2017},
}

@inproceedings{weng_evaluating_2018,
	title = {Evaluating the {Robustness} of {Neural} {Networks}: {An} {Extreme} {Value} {Theory} {Approach}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Weng, Tsui-Wei and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Su, Dong and Gao, Yupeng and Hsieh, Cho-Jui and Daniel, Luca},
	year = {2018},
}

@inproceedings{huster_limitations_2019,
	title = {Limitations of the lipschitz constant as a defense against adversarial examples},
	booktitle = {{ECML} {PKDD} 2018 {Workshops}: {Nemesis} 2018, {UrbReas} 2018, {SoGood} 2018, {IWAISe} 2018, and {Green} {Data} {Mining} 2018, {Dublin}, {Ireland}, {September} 10-14, 2018, {Proceedings} 18},
	publisher = {Springer},
	author = {Huster, Todd and Chiang, Cho-Yu Jason and Chadha, Ritu},
	year = {2019},
	pages = {16--29},
}

@inproceedings{zhang_rethinking_2022,
	title = {Rethinking lipschitz neural networks and certified robustness: {A} boolean function perspective},
	volume = {35},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
	year = {2022},
	pages = {19398--19413},
}

@inproceedings{huang_training_2021,
	title = {Training certifiably robust neural networks with efficient local lipschitz bounds},
	volume = {34},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Huang, Yujia and Zhang, Huan and Shi, Yuanyuan and Kolter, J Zico and Anandkumar, Anima},
	year = {2021},
	pages = {22745--22757},
}

@article{gouk_regularisation_2021,
	title = {Regularisation of neural networks by enforcing lipschitz continuity},
	volume = {110},
	journal = {Machine Learning},
	author = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael J},
	year = {2021},
	note = {Publisher: Springer},
	pages = {393--416},
}

@article{muthukumar_adversarial_2023,
	title = {Adversarial robustness of sparse local lipschitz predictors},
	volume = {5},
	number = {4},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Muthukumar, Ramchandran and Sulam, Jeremias},
	year = {2023},
	note = {Publisher: SIAM},
	pages = {920--948},
}

@article{mcdiarmid_method_1989,
	title = {On the method of bounded differences},
	volume = {141},
	number = {1},
	journal = {Surveys in combinatorics},
	author = {McDiarmid, Colin and {others}},
	year = {1989},
	note = {Publisher: Norwich},
	pages = {148--188},
}

@misc{xu_certifiably_2023,
	title = {Certifiably {Robust} {Transformers} with 1-{Lipschitz} {Self}-{Attention}},
	url = {https://openreview.net/forum?id=hzG72qB0XQ},
	author = {Xu, Xiaojun and Li, Linyi and Cheng, Yu and Mukherjee, Subhabrata and Awadallah, Ahmed Hassan and Li, Bo},
	year = {2023},
}

@inproceedings{wang_certified_2021,
	address = {Online},
	title = {Certified {Robustness} to {Word} {Substitution} {Attack} with {Differential} {Privacy}},
	url = {https://aclanthology.org/2021.naacl-main.87},
	doi = {10.18653/v1/2021.naacl-main.87},
	abstract = {The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we propose WordDP to achieve certified robustness against word substitution at- tacks in text classification via differential privacy (DP). We establish the connection between DP and adversarial robustness for the first time in the text domain and propose a conceptual exponential mechanism-based algorithm to formally achieve the robustness. We further present a practical simulated exponential mechanism that has efficient inference with certified robustness. We not only provide a rigorous analytic derivation of the certified condition but also experimentally compare the utility of WordDP with existing defense algorithms. The results show that WordDP achieves higher accuracy and more than 30X efficiency improvement over the state-of-the-art certified robustness mechanism in typical text classification tasks.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Wenjie and Tang, Pengfei and Lou, Jian and Xiong, Li},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {1102--1112},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
}

@inproceedings{jia_certified_2019,
	address = {Hong Kong, China},
	title = {Certified {Robustness} to {Adversarial} {Word} {Substitutions}},
	url = {https://aclanthology.org/D19-1423},
	doi = {10.18653/v1/D19-1423},
	abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75\% adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI; in comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12\% and 41\%, respectively.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Jia, Robin and Raghunathan, Aditi and Göksel, Kerem and Liang, Percy},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {4129--4142},
}

@inproceedings{li_certified_2019,
	title = {Certified {Adversarial} {Robustness} with {Additive} {Noise}},
	volume = {32},
	booktitle = {Advances in neural information processing systems},
	author = {Li, Bai and Chen, Changyou and Wang, Wenlin and Carin, Lawrence},
	year = {2019},
}

@inproceedings{weng_towards_2018,
	title = {Towards {Fast} {Computation} of {Certified} {Robustness} for {ReLU} {Networks}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Weng, Lily and Zhang, Huan and Chen, Hongge and Song, Zhao and Hsieh, Cho-Jui and Daniel, Luca and Boning, Duane and Dhillon, Inderjit},
	year = {2018},
	pages = {5276--5285},
}

@inproceedings{wong_scaling_2018,
	title = {Scaling {Provable} {Adversarial} {Defenses}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wong, Eric and Schmidt, Frank and Metzen, Jan Hendrik and Kolter, J Zico},
	year = {2018},
}

@inproceedings{wong_provable_2018,
	title = {Provable defenses against adversarial examples via the convex outer adversarial polytope},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Wong, Eric and Kolter, Zico},
	year = {2018},
	pages = {5286--5295},
}

@inproceedings{lecuyer_certified_2019,
	title = {Certified robustness to adversarial examples with differential privacy},
	booktitle = {2019 {IEEE} symposium on security and privacy ({SP})},
	publisher = {IEEE},
	author = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
	year = {2019},
	pages = {656--672},
}

@inproceedings{zhang_orthogonality_2021,
	address = {Online},
	title = {On {Orthogonality} {Constraints} for {Transformers}},
	url = {https://aclanthology.org/2021.acl-short.48},
	doi = {10.18653/v1/2021.acl-short.48},
	abstract = {Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. For example, on the large-scale WMT'16 En→De benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (Vaswani et al., 2017) increases the BLEU from 28.4 to 29.6, coming close to the 29.7 BLEU achieved by the very competitive dynamic convolution (Wu et al., 2019).},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Aston and Chan, Alvin and Tay, Yi and Fu, Jie and Wang, Shuohang and Zhang, Shuai and Shao, Huajie and Yao, Shuochao and Lee, Roy Ka-Wei},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {375--382},
}

@inproceedings{gupta_certvit_2023,
	title = {{CertViT}: {Certified} {Robustness} of {Pre}-{Trained} {Vision} {Transformers}},
	url = {https://openreview.net/forum?id=BSVIgJOwc8},
	booktitle = {The {Second} {Workshop} on {New} {Frontiers} in {Adversarial} {Machine} {Learning}},
	author = {Gupta, Kavya and Verma, Sagar},
	year = {2023},
}

@inproceedings{oren_distributionally_2019,
	address = {Hong Kong, China},
	title = {Distributionally {Robust} {Language} {Modeling}},
	url = {https://aclanthology.org/D19-1432},
	doi = {10.18653/v1/D19-1432},
	abstract = {Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Oren, Yonatan and Sagawa, Shiori and Hashimoto, Tatsunori B. and Liang, Percy},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {4227--4237},
}

@inproceedings{leino_globally-robust_2021,
	title = {Globally-{Robust} {Neural} {Networks}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Leino, Klas and Wang, Zifan and Fredrikson, Matt},
	year = {2021},
	pages = {6212--6222},
}

@inproceedings{wang_infobert_2021,
	title = {Info\{{BERT}\}: {Improving} {Robustness} of {Language} {Models} from {An} {Information} {Theoretic} {Perspective}},
	url = {https://openreview.net/forum?id=hpH98mK5Puk},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
	year = {2021},
}

@inproceedings{zhang_towards_2020,
	title = {Towards {Stable} and {Efficient} {Training} of {Verifiably} {Robust} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Skxuk1rFwB},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Gowal, Sven and Stanforth, Robert and Li, Bo and Boning, Duane and Hsieh, Cho-Jui},
	year = {2020},
}

@inproceedings{zhang_boosting_2022,
	title = {Boosting the {Certified} {Robustness} of {L}-infinity {Distance} {Nets}},
	url = {https://openreview.net/forum?id=Q76Y7wkiji},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
	year = {2022},
}

@inproceedings{anil_sorting_2019,
	title = {Sorting out {Lipschitz} function approximation},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Anil, Cem and Lucas, James and Grosse, Roger},
	year = {2019},
	pages = {291--301},
}

@inproceedings{cisse_parseval_2017,
	title = {Parseval networks: {Improving} robustness to adversarial examples},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
	year = {2017},
	pages = {854--863},
}

@article{yoshida_spectral_2017,
	title = {Spectral norm regularization for improving the generalizability of deep learning},
	journal = {arXiv preprint arXiv:1705.10941},
	author = {Yoshida, Yuichi and Miyato, Takeru},
	year = {2017},
}

@article{gama_stability_2020,
	title = {Stability properties of graph neural networks},
	volume = {68},
	journal = {IEEE Transactions on Signal Processing},
	author = {Gama, Fernando and Bruna, Joan and Ribeiro, Alejandro},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {5680--5695},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@inproceedings{lutjens_certified_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Certified {Adversarial} {Robustness} for {Deep} {Reinforcement} {Learning}},
	volume = {100},
	url = {https://proceedings.mlr.press/v100/lutjens20a.html},
	abstract = {Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was already shown to cause an autonomous vehicle to swerve into oncoming traffic. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certified defense for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose the optimal action under a worst-case deviation in input space due to possible adversaries or noise. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task.},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Lütjens, Björn and Everett, Michael and How, Jonathan P.},
	editor = {Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei},
	month = nov,
	year = {2020},
	pages = {1328--1337},
}

@book{boucheron_concentration_2016,
	title = {Concentration {Inequalities}: {A} {Nonasymptotic} {Theory} of {Independence}},
	isbn = {978-0-19-876765-7},
	publisher = {Oxford University Press},
	author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
	month = apr,
	year = {2016},
}

@inproceedings{chen_this_2019,
	title = {This {Looks} {Like} {That}: {Deep} {Learning} for {Interpretable} {Image} {Recognition}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{radford_learning_2021,
	title = {Learning transferable visual models from natural language supervision},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and {others}},
	year = {2021},
	pages = {8748--8763},
}

@inproceedings{ilyas_adversarial_2019,
	title = {Adversarial {Examples} are not {Bugs}, {They} are {Features}},
	volume = {32},
	booktitle = {Advances in neural information processing systems},
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year = {2019},
}

@inproceedings{moor_topological_2020,
	title = {Topological autoencoders},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Moor, Michael and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
	year = {2020},
	pages = {7045--7054},
}

@inproceedings{shafahi_are_2019,
	title = {Are adversarial examples inevitable?},
	url = {https://openreview.net/forum?id=r1lWUoA9FQ},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Shafahi, Ali and Huang, W. Ronny and Studer, Christoph and Feizi, Soheil and Goldstein, Tom},
	year = {2019},
}

@inproceedings{virmaux_lipschitz_2018,
	title = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Virmaux, Aladin and Scaman, Kevin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@inproceedings{qi_lipsformer_2022,
	title = {{LipsFormer}: {Introducing} {Lipschitz} {Continuity} to {Vision} {Transformers}},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Qi, Xianbiao and Wang, Jianan and Chen, Yihao and Shi, Yukai and Zhang, Lei},
	year = {2022},
}

@inproceedings{durrani_analyzing_2020,
	address = {Online},
	title = {Analyzing {Individual} {Neurons} in {Pre}-trained {Language} {Models}},
	url = {https://aclanthology.org/2020.emnlp-main.395},
	doi = {10.18653/v1/2020.emnlp-main.395},
	abstract = {While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons. We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Durrani, Nadir and Sajjad, Hassan and Dalvi, Fahim and Belinkov, Yonatan},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {4865--4880},
}

@inproceedings{liu_devil_2024,
	title = {The {Devil} is in the {Neurons}: {Interpreting} and {Mitigating} {Social} {Biases} in {Language} {Models}},
	url = {https://openreview.net/forum?id=SQGUDc9tC8},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Liu, Yan and Liu, Yu and Chen, Xiaokang and Chen, Pin-Yu and Zan, Daoguang and Kan, Min-Yen and Ho, Tsung-Yi},
	year = {2024},
}

@inproceedings{rohekar_causal_2023,
	title = {Causal {Interpretation} of {Self}-{Attention} in {Pre}-{Trained} {Transformers}},
	url = {https://openreview.net/forum?id=DS4rKySlYC},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Rohekar, Raanan Yehezkel and Gurwicz, Yaniv and Nisimov, Shami},
	year = {2023},
}

@inproceedings{belkin_overfitting_2018,
	title = {Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Belkin, Mikhail and Hsu, Daniel and Mitra, Partha},
	year = {2018},
}

@inproceedings{nakkiran_deep_2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = oct,
	year = {2020},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine-learning practice and the classical bias-variance trade-off},
	volume = {116},
	number = {32},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	pages = {15849--1584},
}

@inproceedings{lindner_tracr_2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	url = {https://openreview.net/forum?id=tbbId8u7nP},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Lindner, David and Kramar, Janos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	year = {2023},
}

@inproceedings{ni_basisformer_2023,
	title = {{BasisFormer}: {Attention}-based {Time} {Series} {Forecasting} with {Learnable} and {Interpretable} {Basis}},
	url = {https://openreview.net/forum?id=xx3qRKvG0T},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Ni, Zelin and Yu, Hang and Liu, Shizhan and Li, Jianguo and Lin, Weiyao},
	year = {2023},
}

@inproceedings{liu_loss_2020,
	title = {On the {Loss} {Landscape} of {Adversarial} {Training}: {Identifying} {Challenges} and {How} to {Overcome} {Them}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f56d8183992b6c54c92c16a8519a6e2b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Chen and Salzmann, Mathieu and Lin, Tao and Tomioka, Ryota and Süsstrunk, Sabine},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {21476--21487},
}

@inproceedings{lubana_mechanistic_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Mechanistic {Mode} {Connectivity}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/lubana23a.html},
	abstract = {We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model’s mechanisms, e.g., fine-tuning can fail to eliminate a model’s reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model’s mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model’s reliance on spurious attributes.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lubana, Ekdeep Singh and Bigelow, Eric J and Dick, Robert P. and Krueger, David and Tanaka, Hidenori},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {22965--23004},
}

@inproceedings{zhao_bridging_2020,
	title = {Bridging {Mode} {Connectivity} in {Loss} {Landscapes} and {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=SJgwzCEKwH},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhao, Pu and Chen, Pin-Yu and Das, Payel and Ramamurthy, Karthikeyan Natesan and Lin, Xue},
	year = {2020},
}

@misc{shao_adversarial_2022,
	title = {On the {Adversarial} {Robustness} of {Vision} {Transformers}},
	author = {Shao, Rulin and Shi, Zhouxing and Yi, Jinfeng and Chen, Pin-Yu and Hsieh, Cho-Jui},
	year = {2022},
	note = {\_eprint: 2103.15670},
}

@inproceedings{andriushchenko_towards_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Towards {Understanding} {Sharpness}-{Aware} {Minimization}},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/andriushchenko22a.html},
	abstract = {Sharpness-Aware Minimization (SAM) is a recent training method that relies on worst-case weight perturbations which significantly improves generalization in various settings. We argue that the existing justifications for the success of SAM which are based on a PAC-Bayes generalization bound and the idea of convergence to flat minima are incomplete. Moreover, there are no explanations for the success of using m-sharpness in SAM which has been shown as essential for generalization. To better understand this aspect of SAM, we theoretically analyze its implicit bias for diagonal linear networks. We prove that SAM always chooses a solution that enjoys better generalization properties than standard gradient descent for a certain class of problems, and this effect is amplified by using m-sharpness. We further study the properties of the implicit bias on non-linear networks empirically, where we show that fine-tuning a standard model with SAM can lead to significant generalization improvements. Finally, we provide convergence results of SAM for non-convex objectives when used with stochastic gradients. We illustrate these results empirically for deep networks and discuss their relation to the generalization behavior of SAM. The code of our experiments is available at https://github.com/tml-epfl/understanding-sam.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Andriushchenko, Maksym and Flammarion, Nicolas},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	pages = {639--668},
}

@inproceedings{foret_sharpness-aware_2021,
	title = {Sharpness-aware {Minimization} for {Efficiently} {Improving} {Generalization}},
	url = {https://openreview.net/forum?id=6Tm1mposlrM},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	year = {2021},
}

@inproceedings{bahri_sharpness-aware_2022,
	address = {Dublin, Ireland},
	title = {Sharpness-{Aware} {Minimization} {Improves} {Language} {Model} {Generalization}},
	url = {https://aclanthology.org/2022.acl-long.508},
	doi = {10.18653/v1/2022.acl-long.508},
	abstract = {The allure of superhuman-level capabilities has led to considerable interest in language models like GPT-3 and T5, wherein the research has, by and large, revolved around new model architectures, training tasks, and loss objectives, along with substantial engineering efforts to scale up model capacity and dataset size. Comparatively little work has been done to improve the generalization of these models through better optimization. In this work, we show that Sharpness-Aware Minimization (SAM), a recently proposed optimization procedure that encourages convergence to flatter minima, can substantially improve the generalization of language models without much computational overhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web Questions, Natural Questions, Trivia QA, and TyDiQA, with particularly large gains when training data for these tasks is limited.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bahri, Dara and Mobahi, Hossein and Tay, Yi},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {7360--7371},
}

@inproceedings{du_efficient_2022,
	title = {Efficient {Sharpness}-aware {Minimization} for {Improved} {Training} of {Neural} {Networks}},
	url = {https://openreview.net/forum?id=n0OeTdNRG0Q},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Du, Jiawei and Yan, Hanshu and Feng, Jiashi and Zhou, Joey Tianyi and Zhen, Liangli and Goh, Rick Siow Mong and Tan, Vincent},
	year = {2022},
}

@inproceedings{wen_sharpness_2023,
	title = {Sharpness {Minimization} {Algorithms} {Do} {Not} {Only} {Minimize} {Sharpness} {To} {Achieve} {Better} {Generalization}},
	url = {https://openreview.net/forum?id=Dkmpa6wCIx},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Wen, Kaiyue and Li, Zhiyuan and Ma, Tengyu},
	year = {2023},
}

@article{lin_evolutionary-scale_2023,
	title = {Evolutionary-scale prediction of atomic-level protein structure with a language model},
	volume = {379},
	url = {https://www.science.org/doi/abs/10.1126/science.ade2574},
	doi = {10.1126/science.ade2574},
	abstract = {Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for \&gt;617 million metagenomic protein sequences, including \&gt;225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins. Machine learning methods for protein structure prediction have taken advantage of the evolutionary information present in multiple sequence alignments to derive accurate structural information, but predicting structure accurately from a single sequence is much more difficult. Lin et al. trained transformer protein language models with up to 15 billion parameters on experimental and high-quality predicted structures and found that information about atomic-level structure emerged in the model as it was scaled up. They created ESMFold, a sequence-to-structure predictor that is nearly as accurate as alignment-based methods and considerably faster. The increased speed permitted the generation of a database, the ESM Metagenomic Atlas, containing more than 600 million metagenomic proteins. —MAF A protein language model enables structure prediction and analysis of more than 600 million metagenomic proteins.},
	number = {6637},
	journal = {Science},
	author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and Costa, Allan dos Santos and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Salvatore and Rives, Alexander},
	year = {2023},
	note = {\_eprint: https://www.science.org/doi/pdf/10.1126/science.ade2574},
	pages = {1123--1130},
}

@inproceedings{valeriani_geometry_2023,
	title = {The geometry of hidden representations of large transformer models},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a0e66093d7168b40246af1cddc025daa-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Valeriani, Lucrezia and Doimo, Diego and Cuturello, Francesca and Laio, Alessandro and Ansuini, Alessio and Cazzaniga, Alberto},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {51234--51252},
}

@inproceedings{gong_intrinsic_2019,
	title = {On the {Intrinsic} {Dimensionality} of {Image} {Representations}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Gong, Sixue and Boddeti, Vishnu Naresh and Jain, Anil K.},
	month = jun,
	year = {2019},
}

@inproceedings{ansuini_intrinsic_2019,
	title = {Intrinsic dimension of data representations in deep neural networks},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{tulchinskii_intrinsic_2023,
	title = {Intrinsic {Dimension} {Estimation} for {Robust} {Detection} of {AI}-{Generated} {Texts}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/7baa48bc166aa2013d78cbdc15010530-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tulchinskii, Eduard and Kuznetsov, Kristian and Kushnareva, Laida and Cherniavskii, Daniil and Nikolenko, Sergey and Burnaev, Evgeny and Barannikov, Serguei and Piontkovskaya, Irina},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {39257--39276},
}

@inproceedings{kim_distilling_2021,
	title = {Distilling {Robust} and {Non}-{Robust} {Features} in {Adversarial} {Examples} by {Information} {Bottleneck}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/8e5e15c4e6d09c8333a17843461041a9-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Junho and Lee, Byung-Kwan and Ro, Yong Man},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {17148--17159},
}

@misc{shwartz-ziv_representation_2019,
	title = {Representation {Compression} and {Generalization} in {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=SkeL6sCqK7},
	author = {Shwartz-Ziv, Ravid and Painsky, Amichai and Tishby, Naftali},
	year = {2019},
}

@inproceedings{dai_compressing_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Compressing {Neural} {Networks} using the {Variational} {Information} {Bottleneck}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/dai18d.html},
	abstract = {Neural networks can be compressed to reduce memory and computational requirements, or to increase accuracy by facilitating the use of a larger base architecture. In this paper we focus on pruning individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory. To improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound. Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved. In contrast, the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework, providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape. We demonstrate state-of-the-art compression rates across an array of datasets and network architectures.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dai, Bin and Zhu, Chen and Guo, Baining and Wipf, David},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {1135--1144},
}

@inproceedings{alemi_deep_2017,
	title = {Deep {Variational} {Information} {Bottleneck}},
	url = {https://openreview.net/forum?id=HyxQzBceg},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
	year = {2017},
}

@article{shwartz_ziv_compress_2024,
	title = {To {Compress} or {Not} to {Compress}—{Self}-{Supervised} {Learning} and {Information} {Theory}: {A} {Review}},
	volume = {26},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/26/3/252},
	doi = {10.3390/e26030252},
	abstract = {Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory has shaped deep neural networks, particularly the information bottleneck principle. This principle optimizes the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the self-supervised information-theoretic learning problem. This framework includes multiple encoders and decoders, suggesting that all existing work on self-supervised learning can be seen as specific instances. We aim to unify these approaches to understand their underlying principles better and address the main challenge: many works present different frameworks with differing theories that may seem contradictory. By weaving existing research into a cohesive narrative, we delve into contemporary self-supervised methodologies, spotlight potential research areas, and highlight inherent challenges. Moreover, we discuss how to estimate information-theoretic quantities and their associated empirical problems. Overall, this paper provides a comprehensive review of the intersection of information theory, self-supervised learning, and deep neural networks, aiming for a better understanding through our proposed unified approach.},
	number = {3},
	journal = {Entropy},
	author = {Shwartz Ziv, Ravid and LeCun, Yann},
	year = {2024},
}

@article{tuckute_driving_2024,
	title = {Driving and suppressing the human language network using large language models},
	volume = {8},
	issn = {2397-3374},
	url = {https://doi.org/10.1038/s41562-023-01783-7},
	doi = {10.1038/s41562-023-01783-7},
	abstract = {Transformer models such as GPT generate human-like language and are predictive of human brain responses to language. Here, using functional-MRI-measured brain responses to 1,000 diverse sentences, we first show that a GPT-based encoding model can predict the magnitude of the brain response associated with each sentence. We then use the model to identify new sentences that are predicted to drive or suppress responses in the human language network. We show that these model-selected novel sentences indeed strongly drive and suppress the activity of human language areas in new individuals. A systematic analysis of the model-selected sentences reveals that surprisal and well-formedness of linguistic input are key determinants of response strength in the language network. These results establish the ability of neural network models to not only mimic human language but also non-invasively control neural activity in higher-level cortical areas, such as the language network.},
	number = {3},
	journal = {Nature Human Behaviour},
	author = {Tuckute, Greta and Sathe, Aalok and Srikant, Shashank and Taliaferro, Maya and Wang, Mingye and Schrimpf, Martin and Kay, Kendrick and Fedorenko, Evelina},
	month = mar,
	year = {2024},
	pages = {544--561},
}

@inproceedings{geva_transformer_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://aclanthology.org/2021.emnlp-main.446},
	doi = {10.18653/v1/2021.emnlp-main.446},
	abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {5484--5495},
}

@inproceedings{haviv_understanding_2023,
	address = {Dubrovnik, Croatia},
	title = {Understanding {Transformer} {Memorization} {Recall} {Through} {Idioms}},
	url = {https://aclanthology.org/2023.eacl-main.19},
	doi = {10.18653/v1/2023.eacl-main.19},
	abstract = {To produce accurate predictions, language models (LMs) must balance between generalization and memorization. Yet, little is known about the mechanism by which transformer LMs employ their memorization capacity. When does a model decide to output a memorized phrase, and how is this phrase then retrieved from memory? In this work, we offer the first methodological framework for probing and characterizing recall of memorized sequences in transformer LMs. First, we lay out criteria for detecting model inputs that trigger memory recall, and propose idioms as inputs that typically fulfill these criteria. Next, we construct a dataset of English idioms and use it to compare model behavior on memorized vs. non-memorized inputs. Specifically, we analyze the internal prediction construction process by interpreting the model's hidden representations as a gradual refinement of the output probability distribution. We find that across different model sizes and architectures, memorized predictions are a two-step process: early layers promote the predicted token to the top of the output distribution, and upper layers increase model confidence. This suggests that memorized information is stored and retrieved in the early layers of the network. Last, we demonstrate the utility of our methodology beyond idioms in memorized factual statements. Overall, our work makes a first step towards understanding memory recall, and provides a methodological basis for future studies of transformer memorization.},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Haviv, Adi and Cohen, Ido and Gidron, Jacob and Schuster, Roei and Goldberg, Yoav and Geva, Mor},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {248--264},
}

@misc{katz_backward_2024,
	title = {Backward {Lens}: {Projecting} {Language} {Model} {Gradients} into the {Vocabulary} {Space}},
	url = {https://arxiv.org/abs/2402.12865},
	author = {Katz, Shahar and Belinkov, Yonatan and Geva, Mor and Wolf, Lior},
	year = {2024},
	note = {\_eprint: 2402.12865},
}

@inproceedings{senetaire_explainability_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Explainability as statistical inference},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/senetaire23a.html},
	abstract = {A wide variety of model explanation approaches have been proposed in recent years, all guided by very different rationales and heuristics. In this paper, we take a new route and cast interpretability as a statistical inference problem. We propose a general deep probabilistic model designed to produce interpretable predictions. The model’s parameters can be learned via maximum likelihood, and the method can be adapted to any predictor network architecture, and any type of prediction problem. Our model is akin to amortized interpretability methods, where a neural network is used as a selector to allow for fast interpretation at inference time. Several popular interpretability methods are shown to be particular cases of regularized maximum likelihood for our general model. Using our framework, we identify imputation as a common issue of these models. We propose new datasets with ground truth selection which allow for the evaluation of the features importance map and show experimentally that multiple imputation provides more reasonable interpretations.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Senetaire, Hugo Henri Joseph and Garreau, Damien and Frellsen, Jes and Mattei, Pierre-Alexandre},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {30584--30612},
}

@article{schwarzschild_rethinking_2024,
	title = {Rethinking {LLM} {Memorization} through the {Lens} of {Adversarial} {Compression}},
	journal = {arXiv preprint arXiv:2404.15146},
	author = {Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary C and Kolter, J Zico},
	year = {2024},
}

@inproceedings{chen_symbolic_2023,
	title = {Symbolic {Discovery} of {Optimization} {Algorithms}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9a39b4925e35cf447ccba8757137d84f-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {49205--49233},
}

@inproceedings{antonello_scaling_2023,
	title = {Scaling laws for language encoding models in {fMRI}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4533e4a352440a32558c1c227602c323-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Antonello, Richard and Vaidya, Aditya and Huth, Alexander},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {21895--21907},
}

@inproceedings{mirzadeh_relu_2024,
	title = {{ReLU} {Strikes} {Back}: {Exploiting} {Activation} {Sparsity} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=osoWxY8q2E},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Mirzadeh, Seyed Iman and Alizadeh-Vahid, Keivan and Mehta, Sachin and Mundo, Carlo C. del and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
	year = {2024},
}

@inproceedings{wang_enabling_2024,
	title = {Enabling {Lanuguage} {Models} to {Implicitly} {Learn} {Self}-{Improvement}},
	url = {https://openreview.net/forum?id=2tVHNRZuCs},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Ziqi and Hou, Le and Lu, Tianjian and Wu, Yuexin and Li, Yunxuan and Yu, Hongkun and Ji, Heng},
	year = {2024},
}

@inproceedings{pang_frozen_2024,
	title = {Frozen {Transformers} in {Language} {Models} {Are} {Effective} {Visual} {Encoder} {Layers}},
	url = {https://openreview.net/forum?id=t0FI3Q66K5},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Pang, Ziqi and Xie, Ziyang and Man, Yunze and Wang, Yu-Xiong},
	year = {2024},
}

@inproceedings{deletang_language_2024,
	title = {Language {Modeling} {Is} {Compression}},
	url = {https://openreview.net/forum?id=jznbgiynus},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Deletang, Gregoire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and Hutter, Marcus and Veness, Joel},
	year = {2024},
}

@inproceedings{richens_robust_2024,
	title = {Robust agents learn causal world models},
	url = {https://openreview.net/forum?id=pOoKI3ouv1},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Richens, Jonathan and Everitt, Tom},
	year = {2024},
}

@inproceedings{staab_beyond_2024,
	title = {Beyond {Memorization}: {Violating} {Privacy} via {Inference} with {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=kmn0BhQk7p},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Staab, Robin and Vero, Mark and Balunovic, Mislav and Vechev, Martin},
	year = {2024},
}

@inproceedings{rubin_grokking_2024,
	title = {Grokking as a {First} {Order} {Phase} {Transition} in {Two} {Layer} {Networks}},
	url = {https://openreview.net/forum?id=3ROGsTX3IR},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Rubin, Noa and Seroussi, Inbar and Ringel, Zohar},
	year = {2024},
}

@inproceedings{lee_mechanistic_2024,
	title = {A {Mechanistic} {Understanding} of {Alignment} {Algorithms}: {A} {Case} {Study} on {DPO} and {Toxicity}},
	url = {https://openreview.net/forum?id=dBqHGZPGZI},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin and Kummerfeld, Jonathan K. and Mihalcea, Rada},
	year = {2024},
}

@inproceedings{geva_transformer_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://aclanthology.org/2022.emnlp-main.3},
	doi = {10.18653/v1/2022.emnlp-main.3},
	abstract = {Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50\%, and for improving computation efficiency with a simple early exit rule, saving 20\% of computation on average.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {30--45},
}

@article{mcleish_transformers_2024,
	title = {Transformers {Can} {Do} {Arithmetic} with the {Right} {Embeddings}},
	journal = {arXiv preprint arXiv:2405.17399},
	author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and {others}},
	year = {2024},
}

@article{carlini_evaluating_2019,
	title = {On evaluating adversarial robustness},
	journal = {arXiv preprint arXiv:1902.06705},
	author = {Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
	year = {2019},
}

@article{heimersheim_how_2024,
	title = {How to use and interpret activation patching},
	journal = {arXiv preprint arXiv:2404.15255},
	author = {Heimersheim, Stefan and Nanda, Neel},
	year = {2024},
}

@article{rushing_explorations_2024,
	title = {Explorations of {Self}-{Repair} in {Language} {Models}},
	journal = {arXiv preprint arXiv:2402.15390},
	author = {Rushing, Cody and Nanda, Neel},
	year = {2024},
}

@article{mcgrath_hydra_2023,
	title = {The hydra effect: {Emergent} self-repair in language model computations},
	journal = {arXiv preprint arXiv:2307.15771},
	author = {McGrath, Thomas and Rahtz, Matthew and Kramar, Janos and Mikulik, Vladimir and Legg, Shane},
	year = {2023},
}

@inproceedings{kornblith_similarity_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/kornblith19a.html},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {3519--3529},
}

@article{elazar_amnesic_2021,
	title = {Amnesic {Probing}: {Behavioral} {Explanation} with {Amnesic} {Counterfactuals}},
	volume = {9},
	url = {https://aclanthology.org/2021.tacl-1.10},
	doi = {10.1162/tacl_a_00359},
	abstract = {A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, for example, is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.1},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2021},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {160--175},
}

@inproceedings{qi_fine-tuning_2024,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},
	url = {https://openreview.net/forum?id=hTEGyKf0dZ},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
	year = {2024},
}

@article{feder_causalm_2021,
	title = {{CausaLM}: {Causal} {Model} {Explanation} {Through} {Counterfactual} {Language} {Models}},
	volume = {47},
	url = {https://aclanthology.org/2021.cl-2.13},
	doi = {10.1162/coli_a_00404},
	abstract = {Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning–based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1},
	number = {2},
	journal = {Computational Linguistics},
	author = {Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
	month = jun,
	year = {2021},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {333--386},
}

@inproceedings{burns_weak--strong_2024,
	title = {Weak-to-{Strong} {Generalization}: {Eliciting} {Strong} {Capabilities} {With} {Weak} {Supervision}},
	url = {https://openreview.net/forum?id=ghNRg2mEgN},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and Sutskever, Ilya and Wu, Jeffrey},
	year = {2024},
}

@inproceedings{amos_never_2024,
	title = {Never {Train} from {Scratch}: {Fair} {Comparison} of {Long}-{Sequence} {Models} {Requires} {Data}-{Driven} {Priors}},
	url = {https://openreview.net/forum?id=PdaPky8MUn},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Amos, Ido and Berant, Jonathan and Gupta, Ankit},
	year = {2024},
}

@inproceedings{darcet_vision_2024,
	title = {Vision {Transformers} {Need} {Registers}},
	url = {https://openreview.net/forum?id=2dnO3LLiJ1},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
	year = {2024},
}

@inproceedings{reddy_mechanistic_2024,
	title = {The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
	url = {https://openreview.net/forum?id=aN4Jf6Cx69},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Reddy, Gautam},
	year = {2024},
}

@article{sanford_representational_2024,
	title = {Representational strengths and limitations of transformers},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sanford, Clayton and Hsu, Daniel J and Telgarsky, Matus},
	year = {2024},
}

@article{liu_kan_2024,
	title = {Kan: {Kolmogorov}-arnold networks},
	journal = {arXiv preprint arXiv:2404.19756},
	author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y and Tegmark, Max},
	year = {2024},
}

@article{wei_statistically_2022,
	title = {Statistically meaningful approximation: a case study on approximating turing machines with transformers},
	volume = {35},
	journal = {Advances in Neural Information Processing Systems},
	author = {Wei, Colin and Chen, Yining and Ma, Tengyu},
	year = {2022},
	pages = {12071--12083},
}

@inproceedings{wightman_resnet_2021,
	title = {{ResNet} strikes back: {An} improved training procedure in timm},
	url = {https://openreview.net/forum?id=NG6MJnVl6M5},
	booktitle = {{NeurIPS} 2021 {Workshop} on {ImageNet}: {Past}, {Present}, and {Future}},
	author = {Wightman, Ross and Touvron, Hugo and Jegou, Herve},
	year = {2021},
}

@misc{arditi_refusal_2024,
	title = {Refusal in {Language} {Models} {Is} {Mediated} by a {Single} {Direction}},
	url = {https://arxiv.org/abs/2406.11717},
	author = {Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
	year = {2024},
	note = {\_eprint: 2406.11717},
}

@article{alkhamissi_brain-like_2024,
	title = {Brain-like language processing via a shallow untrained multihead attention network},
	journal = {arXiv preprint arXiv:2406.15109},
	author = {AlKhamissi, Badr and Tuckute, Greta and Bosselut, Antoine and Schrimpf, Martin},
	year = {2024},
}

@misc{barbero_transformers_2024,
	title = {Transformers need glasses! {Information} over-squashing in language tasks},
	url = {https://arxiv.org/abs/2406.04267},
	author = {Barbero, Federico and Banino, Andrea and Kapturowski, Steven and Kumaran, Dharshan and Araújo, João G. M. and Vitvitskyi, Alex and Pascanu, Razvan and Veličković, Petar},
	year = {2024},
	note = {\_eprint: 2406.04267},
}

@article{shwartz-ziv_simplifying_2024,
	title = {Simplifying neural network training under class imbalance},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Shwartz-Ziv, Ravid and Goldblum, Micah and Li, Yucen and Bruss, C Bayan and Wilson, Andrew G},
	year = {2024},
}

@article{cusack_helpless_nodate,
	title = {Helpless infants are learning a foundation model},
	url = {https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(24)00114-1},
	journal = {Trends in Cognitive Sciences},
	author = {Cusack, Rhodri and Ranzato, Marc’Aurelio and Charvet, Christine J},
	note = {Publisher: Elsevier},
}

@inproceedings{zhao_probabilistic_2024,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Probabilistic {Inference} in {Language} {Models} via {Twisted} {Sequential} {Monte} {Carlo}},
	volume = {235},
	url = {https://proceedings.mlr.press/v235/zhao24c.html},
	abstract = {Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhao, Stephen and Brekelmans, Rob and Makhzani, Alireza and Grosse, Roger Baker},
	editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	month = jul,
	year = {2024},
	pages = {60704--60748},
}

@article{ma_era_2024,
	title = {The era of 1-bit llms: {All} large language models are in 1.58 bits},
	journal = {arXiv preprint arXiv:2402.17764},
	author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
	year = {2024},
}

@inproceedings{press_entropy_2024,
	title = {The {Entropy} {Enigma}: {Success} and {Failure} of {Entropy} {Minimization}},
	url = {https://openreview.net/forum?id=0bGsVoumFL},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Press, Ori and Shwartz-Ziv, Ravid and LeCun, Yann and Bethge, Matthias},
	year = {2024},
}

@inproceedings{dong_attention_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/dong21a.html},
	abstract = {Attention-based architectures have become ubiquitous in machine learning. Yet, our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms—or paths—each involving the operation of a sequence of attention heads across layers. Using this path decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the convergence results on standard transformer architectures.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {2793--2803},
}

@inproceedings{zhai_stabilizing_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Stabilizing {Transformer} {Training} by {Preventing} {Attention} {Entropy} {Collapse}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/zhai23a.html},
	abstract = {Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as \textit{entropy collapse}. As a remedy, we propose σReparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that σReparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with σReparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that σReparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer to competitive performance without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at https://github.com/apple/ml-sigma-reparam.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {40770--40803},
}

@inproceedings{chen_why_2023,
	title = {Why {Does} {Sharpness}-{Aware} {Minimization} {Generalize} {Better} {Than} {SGD}?},
	url = {https://openreview.net/forum?id=3WAnGWLpSQ},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Zixiang and Zhang, Junkai and Kou, Yiwen and Chen, Xiangning and Hsieh, Cho-Jui and Gu, Quanquan},
	year = {2023},
}

@inproceedings{liu_dora_2024,
	title = {{DoRA}: {Weight}-{Decomposed} {Low}-{Rank} {Adaptation}},
	url = {https://openreview.net/forum?id=3d5CIRG1n2},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Liu, Shih-yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
	year = {2024},
}

@inproceedings{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@inproceedings{garipov_loss_2018,
	title = {Loss {Surfaces}, {Mode} {Connectivity}, and {Fast} {Ensembling} of {DNNs}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@inproceedings{yang_taxonomizing_2021,
	title = {Taxonomizing local versus global structure in neural network loss landscapes},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/9b72e31dac81715466cd580a448cf823-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Yaoqing and Hodgkinson, Liam and Theisen, Ryan and Zou, Joe and Gonzalez, Joseph E and Ramchandran, Kannan and Mahoney, Michael W},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {18722--18733},
}

@inproceedings{schaeffer_are_2023,
	title = {Are {Emergent} {Abilities} of {Large} {Language} {Models} a {Mirage}?},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/adc98a266f45005c403b8311ca7e8bd7-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {55565--55581},
}

@article{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=yzkSU5zdwD},
	journal = {Transactions on Machine Learning Research},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	year = {2022},
}

@article{qiu_dissecting_2024,
	title = {Dissecting {Multiplication} in {Transformers}: {Insights} into {LLMs}},
	journal = {arXiv preprint arXiv:2407.15360},
	author = {Qiu, Luyu and Li, Jianing and Su, Chi and Zhang, Chen Jason and Chen, Lei},
	year = {2024},
}

@inproceedings{hendrycks_measuring_2021,
	title = {Measuring {Mathematical} {Problem} {Solving} {With} the {MATH} {Dataset}},
	url = {https://openreview.net/forum?id=7Bywt2mQsCe},
	booktitle = {Thirty-fifth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track} ({Round} 2)},
	author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	year = {2021},
}

@inproceedings{mao_wamp_2023,
	title = {{WAMP}: {A} {Competition}-{Level} {Dataset} for {Assessing} the {Mathematical} {Reasoning} {Capabilities} of {LLMs}},
	url = {https://openreview.net/forum?id=loDIYsNLbw},
	booktitle = {The 3rd {Workshop} on {Mathematical} {Reasoning} and {AI} at {NeurIPS}'23},
	author = {Mao, Yujun and Kim, Yoon and Zhou, Yilun},
	year = {2023},
}

@inproceedings{steinke_privacy_2023,
	title = {Privacy {Auditing} with {One} (1) {Training} {Run}},
	url = {https://openreview.net/forum?id=f38EY21lBw},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Steinke, Thomas and Nasr, Milad and Jagielski, Matthew},
	year = {2023},
}

@inproceedings{sun_algorithmic_2024,
	title = {Algorithmic {Phase} {Transitions} in {Language} {Models}: {A} {Mechanistic} {Case} {Study} of {Arithmetic}},
	booktitle = {submission at {NeurIPS} 2024 {Workshop} on {Attributing} {Model} {Behavior} at {Scale}},
	author = {Sun, Alan and Sun, Ethan and Shepard, Warren},
	month = sep,
	year = {2024},
}

@inproceedings{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {24824--24837},
}

@article{bousquet_stability_2002,
	title = {Stability and {Generalization}},
	volume = {2},
	journal = {Journal of Machine Learning Research},
	author = {Bousquet, Olivier and Elisseeff, André},
	year = {2002},
	pages = {499--526},
}

@inproceedings{sun_achieving_2024,
	title = {Achieving {Domain}-{Independent} {Certified} {Robustness} via {Knowledge} {Continuity}},
	booktitle = {submission at {NeurIPS} 2024},
	author = {Sun, Alan and Ma, Chiyu and Ge, Kenneth and Vosoughi, Soroush},
	year = {2024},
}

@article{bilodeau_impossibility_2024,
	title = {Impossibility theorems for feature attribution},
	volume = {121},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2304406120},
	doi = {10.1073/pnas.2304406120},
	abstract = {Machine learning models can learn complex patterns from data, but it is often difficult to understand why they make particular predictions. To tackle this problem, practitioners typically turn to feature attribution methods, which seek to attribute the model’s behavior f(x) around an example x to particular features, or dimensions of x, that are most important for the prediction. In recent years, a new class of feature attribution methods—namely, complete and linear methods—has become popular. Our work shows that, unfortunately, such methods can be misleading: Complete and linear methods are provably less reliable than simpler methods at answering basic feature attribution questions. We provide impossibility results that highlight their failure cases and discuss how we might instead obtain reliable feature attributions. Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear—for example, Integrated Gradients and Shapley Additive Explanations (SHAP)—can provably fail to improve on random guessing for inferring model behavior. Our results apply to common end-tasks such as characterizing local model behavior, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: Once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.},
	number = {2},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bilodeau, Blair and Jaques, Natasha and Koh, Pang Wei and Kim, Been},
	year = {2024},
	note = {\_eprint: https://www.pnas.org/doi/pdf/10.1073/pnas.2304406120},
	pages = {e2304406120},
}

@article{elhage_toy_2022,
	title = {Toy models of superposition},
	journal = {arXiv preprint arXiv:2209.10652},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and {others}},
	year = {2022},
}

@article{bietti_birth_2024,
	title = {Birth of a transformer: {A} memory viewpoint},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
	year = {2024},
}

@article{fisher_all_2019,
	title = {All {Models} are {Wrong}, but {Many} are {Useful}: {Learning} a {Variable}'s {Importance} by {Studying} an {Entire} {Class} of {Prediction} {Models} {Simultaneously}},
	volume = {20},
	url = {http://jmlr.org/papers/v20/18-760.html},
	number = {177},
	journal = {Journal of Machine Learning Research},
	author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
	year = {2019},
	pages = {1--81},
}

@article{cunningham_sparse_2023,
	title = {Sparse autoencoders find highly interpretable features in language models},
	journal = {arXiv preprint arXiv:2309.08600},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	year = {2023},
}

@article{bhaskar_finding_2024,
	title = {Finding {Transformer} {Circuits} with {Edge} {Pruning}},
	journal = {arXiv preprint arXiv:2406.16778},
	author = {Bhaskar, Adithya and Wettig, Alexander and Friedman, Dan and Chen, Danqi},
	year = {2024},
}

@article{fisch_calibrated_2022,
	title = {Calibrated {Selective} {Classification}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=zFhNBs8GaV},
	journal = {Transactions on Machine Learning Research},
	author = {Fisch, Adam and Jaakkola, Tommi S. and Barzilay, Regina},
	year = {2022},
}

@misc{lecomte_what_2024,
	title = {What {Causes} {Polysemanticity}? {An} {Alternative} {Origin} {Story} of {Mixed} {Selectivity} from {Incidental} {Causes}},
	url = {https://arxiv.org/abs/2312.03096},
	author = {Lecomte, Victor and Thaman, Kushal and Schaeffer, Rylan and Bashkansky, Naomi and Chow, Trevor and Koyejo, Sanmi},
	year = {2024},
	note = {\_eprint: 2312.03096},
}

@article{mickus_isotropy_2024,
	title = {Isotropy, clusters, and classifiers},
	journal = {arXiv preprint arXiv:2402.03191},
	author = {Mickus, Timothee and Grönroos, Stig-Arne and Attieh, Joseph},
	year = {2024},
}

@article{kallini_mission_2024,
	title = {Mission: {Impossible} language models},
	journal = {arXiv preprint arXiv:2401.06416},
	author = {Kallini, Julie and Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle and Potts, Christopher},
	year = {2024},
}

@article{gambardella_language_2024,
	title = {Language {Models} {Do} {Hard} {Arithmetic} {Tasks} {Easily} and {Hardly} {Do} {Easy} {Arithmetic} {Tasks}},
	journal = {arXiv preprint arXiv:2406.02356},
	author = {Gambardella, Andrew and Iwasawa, Yusuke and Matsuo, Yutaka},
	year = {2024},
}

@article{phuong_formal_2022,
	title = {Formal algorithms for transformers},
	journal = {arXiv preprint arXiv:2207.09238},
	author = {Phuong, Mary and Hutter, Marcus},
	year = {2022},
}

@inproceedings{gurnee_language_2024,
	title = {Language {Models} {Represent} {Space} and {Time}},
	url = {https://openreview.net/forum?id=jE8xbmvFin},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Gurnee, Wes and Tegmark, Max},
	year = {2024},
}

@article{kuzmin_pruning_2024,
	title = {Pruning vs quantization: which is better?},
	volume = {36},
	journal = {Advances in neural information processing systems},
	author = {Kuzmin, Andrey and Nagel, Markus and Van Baalen, Mart and Behboodi, Arash and Blankevoort, Tijmen},
	year = {2024},
}

@inproceedings{park_linear_2024,
	title = {The {Linear} {Representation} {Hypothesis} and the {Geometry} of {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=UGpGkLzwpP},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
	year = {2024},
}

@inproceedings{patel_mapping_2022,
	title = {Mapping {Language} {Models} to {Grounded} {Conceptual} {Spaces}},
	url = {https://openreview.net/forum?id=gJcEM8sxHK},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Patel, Roma and Pavlick, Ellie},
	year = {2022},
}

@inproceedings{weiss_thinking_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Thinking {Like} {Transformers}},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/weiss21a.html},
	abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {11080--11090},
}

@inproceedings{friedman_learning_2023,
	title = {Learning {Transformer} {Programs}},
	url = {https://openreview.net/forum?id=Pe9WxkN8Ff},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Friedman, Dan and Wettig, Alexander and Chen, Danqi},
	year = {2023},
}

@article{gupta_interpbench_2024,
	title = {Interpbench: {Semi}-synthetic transformers for evaluating mechanistic interpretability techniques},
	journal = {arXiv preprint arXiv:2407.14494},
	author = {Gupta, Rohan and Arcuschin, Iván and Kwa, Thomas and Garriga-Alonso, Adrià},
	year = {2024},
}

@article{thurnherr_tracrbench_2024,
	title = {Tracrbench: {Generating} interpretability testbeds with large language models},
	journal = {arXiv preprint arXiv:2409.13714},
	author = {Thurnherr, Hannes and Scheurer, Jérémy},
	year = {2024},
}

@article{zennaro_abstraction_2022,
	title = {Abstraction between structural causal models: {A} review of definitions and properties},
	journal = {arXiv preprint arXiv:2207.08603},
	author = {Zennaro, Fabio Massimo},
	year = {2022},
}

@inproceedings{beckers_abstracting_2019,
	title = {Abstracting causal models},
	volume = {33},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Beckers, Sander and Halpern, Joseph Y},
	year = {2019},
	note = {Issue: 01},
	pages = {2678--2685},
}

@inproceedings{massidda_causal_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Causal {Abstraction} with {Soft} {Interventions}},
	volume = {213},
	url = {https://proceedings.mlr.press/v213/massidda23a.html},
	abstract = {Causal abstraction provides a theory describing how several causal models can represent the same system at different levels of detail. Existing theoretical proposals limit the analysis of abstract models to "hard" interventions fixing causal variables to be constant values. In this work, we extend causal abstraction to "soft" interventions, which assign possibly non-constant functions to variables without adding new causal connections. Specifically, (i) we generalize τ-abstraction from Beckers and Halpern (2019) to soft interventions, (ii) we propose a further definition of soft abstraction to ensure a unique map ømega between soft interventions, and (iii) we prove that our constructive definition of soft abstraction guarantees the intervention map ømega has a specific and necessary explicit form.},
	booktitle = {Proceedings of the {Second} {Conference} on {Causal} {Learning} and {Reasoning}},
	publisher = {PMLR},
	author = {Massidda, Riccardo and Geiger, Atticus and Icard, Thomas and Bacciu, Davide},
	editor = {van der Schaar, Mihaela and Zhang, Cheng and Janzing, Dominik},
	month = apr,
	year = {2023},
	pages = {68--87},
}

@inproceedings{rischel_compositional_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Compositional abstraction error and a category of causal models},
	volume = {161},
	url = {https://proceedings.mlr.press/v161/rischel21a.html},
	abstract = {Interventional causal models describe several joint distributions over some variables used to describe a system, one for each intervention setting. They provide a formal recipe for how to move between the different joint distributions and make predictions about the variables upon intervening on the system. Yet, it is difficult to formalise how we may change the underlying variables used to describe the system, say moving from fine-grained to coarse-grained variables. Here, we argue that compositionality is a desideratum for such model transformations and the associated errors: When abstracting a reference model M iteratively, first obtaining M’ and then further simplifying that to obtain M”, we expect the composite transformation from M to M” to exist and its error to be bounded by the errors incurred by each individual transformation step. Category theory, the study of mathematical objects via compositional transformations between them, offers a natural language to develop our framework for model transformations and abstractions. We introduce a category of finite interventional causal models and, leveraging theory of enriched categories, prove the desired compositionality properties for our framework.},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Rischel, Eigil F. and Weichwald, Sebastian},
	editor = {de Campos, Cassio and Maathuis, Marloes H.},
	month = jul,
	year = {2021},
	pages = {1013--1023},
}

@inproceedings{otsuka_equivalence_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On the {Equivalence} of {Causal} {Models}: {A} {Category}-{Theoretic} {Approach}},
	volume = {177},
	url = {https://proceedings.mlr.press/v177/otsuka22a.html},
	abstract = {We develop a category-theoretic criterion for determining the equivalence of causal models having different but homomorphic directed acyclic graphs over discrete variables. Following Jacobs et al. (2019), we define a causal model as a probabilistic interpretation of a causal string diagram, i.e., a functor from the “syntactic” category Syn\_G of graph G to the category Stoch of finite sets and stochastic matrices. The equivalence of causal models is then defined in terms of a natural transformation or isomorphism between two such functors, which we call a Φ-abstraction and Φ-equivalence, respectively. It is shown that when one model is a Φ-abstraction of another, the intervention calculus of the former can be consistently translated into that of the latter. We also identify the condition under which a model accommodates a Φ-abstraction, when transformations are deterministic.},
	booktitle = {Proceedings of the {First} {Conference} on {Causal} {Learning} and {Reasoning}},
	publisher = {PMLR},
	author = {Otsuka, Jun and Saigo, Hayato},
	editor = {Schölkopf, Bernhard and Uhler, Caroline and Zhang, Kun},
	month = apr,
	year = {2022},
	pages = {634--646},
}

@article{jacobs_causal_2021,
	title = {Causal inference via string diagram surgery: {A} diagrammatic approach to interventions and counterfactuals},
	volume = {31},
	number = {5},
	journal = {Mathematical Structures in Computer Science},
	author = {Jacobs, Bart and Kissinger, Aleks and Zanasi, Fabio},
	year = {2021},
	note = {Publisher: Cambridge University Press},
	pages = {553--574},
}

@book{shalev-shwartz_understanding_2014,
	title = {Understanding machine learning: {From} theory to algorithms},
	publisher = {Cambridge university press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
}

@inproceedings{tigges_llm_2024,
	title = {{LLM} {Circuit} {Analyses} {Are} {Consistent} {Across} {Training} and {Scale}},
	url = {https://openreview.net/forum?id=3Ds5vNudIE},
	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Tigges, Curt and Hanna, Michael and Yu, Qinan and Biderman, Stella},
	year = {2024},
}

@inproceedings{hanna_have_2024,
	title = {Have {Faith} in {Faithfulness}: {Going} {Beyond} {Circuit} {Overlap} {When} {Finding} {Model} {Mechanisms}},
	url = {https://openreview.net/forum?id=TZ0CCGDcuT},
	booktitle = {First {Conference} on {Language} {Modeling}},
	author = {Hanna, Michael and Pezzelle, Sandro and Belinkov, Yonatan},
	year = {2024},
}

@inproceedings{suzgun_challenging_2023,
	address = {Toronto, Canada},
	title = {Challenging {BIG}-{Bench} {Tasks} and {Whether} {Chain}-of-{Thought} {Can} {Solve} {Them}},
	url = {https://aclanthology.org/2023.findings-acl.824/},
	doi = {10.18653/v1/2023.findings-acl.824},
	abstract = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65\% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and Wei, Jason},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {13003--13051},
}

@misc{friedman_interpretability_2024,
	title = {Interpretability {Illusions} in the {Generalization} of {Simplified} {Models}},
	url = {https://openreview.net/forum?id=v675Iyu0ta},
	author = {Friedman, Dan and Lampinen, Andrew Kyle and Dixon, Lucas and Chen, Danqi and Ghandeharioun, Asma},
	year = {2024},
}

@article{nanda_attribution_2023,
	title = {Attribution patching: {Activation} patching at industrial scale},
	journal = {URL: https://www. neelnanda. io/mechanistic-interpretability/attribution-patching},
	author = {Nanda, Neel},
	year = {2023},
}

@article{syed_attribution_2023,
	title = {Attribution patching outperforms automated circuit discovery},
	journal = {arXiv preprint arXiv:2310.10348},
	author = {Syed, Aaquib and Rager, Can and Conmy, Arthur},
	year = {2023},
}

@inproceedings{giannou_looped_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Looped {Transformers} as {Programmable} {Computers}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/giannou23a.html},
	abstract = {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches. Using this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms to programs that can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-Yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {11398--11442},
}

@article{merrill_saturated_2022,
	title = {Saturated {Transformers} are {Constant}-{Depth} {Threshold} {Circuits}},
	volume = {10},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl\_a\_00493},
	doi = {10.1162/tacl_a_00493},
	abstract = {Transformers have become a standard neural network architecture for many NLP problems, motivating theoretical analysis of their power in terms of formal languages. Recent work has shown that transformers with hard attention are quite limited in power (Hahn, 2020), as they can be simulated by constant-depth AND/OR circuits (Hao et al., 2022). However, hard attention is a strong assumption, which may complicate the relevance of these results in practice. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We first show that saturated transformers transcend the known limitations of hard-attention transformers. We then prove saturated transformers with floating-point values can be simulated by constant-depth threshold circuits, giving the class TC0 as an upper bound on the formal languages they recognize.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Merrill, William and Sabharwal, Ashish and Smith, Noah A.},
	month = aug,
	year = {2022},
	note = {\_eprint: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00493/2038506/tacl\_a\_00493.pdf},
	pages = {843--856},
}

@inproceedings{jang_categorical_2017,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	url = {https://openreview.net/forum?id=rkE3y85ee},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	year = {2017},
}
